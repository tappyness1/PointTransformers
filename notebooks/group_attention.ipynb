{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)    \n",
    "\n",
    "softmax_0d = nn.Softmax(dim = 0)\n",
    "softmax_1d = nn.Softmax(dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = [[4,5,6,7]]\n",
    "# gamma = np.array(gamma).astype('float32')\n",
    "gamma = torch.tensor(gamma).to(torch.float32)\n",
    "\n",
    "v = [[0.8, 0.2, 0.5, 0.1]]\n",
    "v = torch.tensor(v).to(torch.float32)\n",
    "\n",
    "# in all cases, except VA, you want your V to be in dim 2x2. This assumed only one data point goes in\n",
    "v = v.reshape(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped Vector Attention\n",
    "\n",
    "As written in the paper:\n",
    "\n",
    "![GVA](images/GVA_paper.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works out\n",
    "\n",
    "Won't touch on Vector Attention, since that was so [2020](https://arxiv.org/abs/2012.09164). Below is how I think the rest of the algorithms work. \n",
    "\n",
    "Note the Linear Layer in GVA-Linear weights are given b:\n",
    "\n",
    "```\n",
    "[[4.0, 4.0, 2.0, 1.0],\n",
    " [5.0, 6.0, 1.0, 2.0]]\n",
    "```\n",
    "\n",
    "Also do note that the the numbers in the middle layer were not softmax'd. Don't worry though. The numbers do check out in the workings below. \n",
    "\n",
    "![GVA writings](images/GVA_written_out.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GVA - Linear \n",
    "\n",
    "Easy implementation. Don't need to tinker with the weights to reshape. Just need to reshape out before hadamard with v. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = nn.Linear(4,2, bias = False).to(torch.float32)\n",
    "weights.weight = torch.nn.Parameter(torch.tensor([[4.0,4.0,2.0,1.0],[5.0, 6.0, 1.0, 2.0]])).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = weights(gamma)\n",
    "out = out.reshape(2,1)\n",
    "out\n",
    "# softmax here before do hadamard with the v\n",
    "out = softmax_0d(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4472e-07, 6.1180e-08],\n",
       "        [5.0000e-01, 1.0000e-01]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hadamard product\n",
    "out*v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GVA - MSA\n",
    "\n",
    "The \"easiest\" because all you do is reshape gamma and sum them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0558],\n",
       "        [0.9442]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_0d = nn.Softmax(dim = 0)\n",
    "out_msa = gamma.reshape(2,2)\n",
    "out_msa = torch.sum(out_msa, dim = 1)\n",
    "out_msa = out_msa.reshape(2,1)\n",
    "out_msa *= 1 / np.sqrt(2) # we have 4 channel features, 2 groups. Hence 2\n",
    "out_msa = softmax_0d(out_msa)\n",
    "out_msa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0446, 0.0112],\n",
       "        [0.4721, 0.0944]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_msa * v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GVA - Grouped Linear\n",
    "\n",
    "Solution - use 1D convolution, groups = 2 or how many you want to specify. They don't appear to share weights so it should work out right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_weights = nn.Conv1d(4,2, 1, groups = 2, bias = False)\n",
    "conv_weights.weight = torch.nn.Parameter(torch.tensor([[[4.0],[4.0]],[[5.0],[6.0]]])).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[36.],\n",
       "        [72.]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_gl = conv_weights(gamma.unsqueeze(0).permute(0,2,1))\n",
    "out_gl = out_gl.squeeze(-1).transpose(1,0)\n",
    "out_gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8556e-16, 4.6390e-17],\n",
       "        [5.0000e-01, 1.0000e-01]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_gl = softmax_0d(out_gl)\n",
    "out_gl * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate pointclouds\n",
    "point_clouds = torch.randn(1, 1, 4)\n",
    "out_gl = conv_weights(point_clouds.permute(0,2,1))\n",
    "out_gl = softmax_1d(out_gl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7315, 0.0171],\n",
       "         [0.4572, 0.0086]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_gl = out_gl.permute(0,2,1)\n",
    "out_gl * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.PTv2.ptv2_utils import GroupVectorAttention, PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 14, 4])\n",
      "torch.Size([1, 16, 14, 4])\n"
     ]
    }
   ],
   "source": [
    "gva = GroupVectorAttention(4, 2, 2)\n",
    "points = torch.randn(1, 16, 7)\n",
    "neighbours = torch.randn(1, 16, 14, 7)\n",
    "out = gva(points[..., :3], points[...,3:], neighbours[..., :3], neighbours[...,3:])\n",
    "print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
